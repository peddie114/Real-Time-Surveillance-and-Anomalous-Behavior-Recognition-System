loss,accuracy=model.evaluate(test_seq)
loss=history.history["loss"]
epochs=range(1,len(loss)+1)
val_loss=history.history["val_loss"]
plt.plot(epochs,loss,"bo-",label="traning loss")
plt.plot(epochs,val_loss,"ro-",label="validation loss")
plt.title("training and validation")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(loc=0)
plt.show()


ACC=history.history["accuracy"]
epochs=range(1,len(ACC)+1)
val_acc = history.history["val_accuracy"]
plt.plot(epochs, ACC, "bo-", label="Training Acc")
plt.plot(epochs, val_acc, "ro--", label="Validation Acc")
plt.title("Training and Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(loc=0)
plt.show()

for X_batch, Y_batch in test_seq:
    preds = model.predict(X_batch)  # preds shape: (batch_size, 1)
    print("predictive probability (raw):", preds.flatten())
    break

unique, counts = np.unique(Y_train, return_counts=True)
print("Training Labelingï¼š", dict(zip(unique, counts)))


y_pred_all = []
y_true_all = []

for X_batch, Y_batch in test_seq:
    preds = model.predict(X_batch)  # preds shape: (batch_size, 1)

    y_pred_batch = (preds.flatten() >= 0.4).astype(int)
    y_pred_all.append(y_pred_batch)
    y_true_all.append(Y_batch.flatten())

y_pred_all = np.concatenate(y_pred_all, axis=0)
y_true_all = np.concatenate(y_true_all, axis=0)

conf_mat = confusion_matrix(y_true_all, y_pred_all)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=["Normal", "Abnormal"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

print("y_pred_all:", y_pred_all)
print("y_true_all:", y_true_all)


print("Confusion Matrix:")
cm=confusion_matrix(y_true_all,y_pred_all)
print(cm)

report=classification_report(y_true_all,y_pred_all,target_names=["Normal","Abnormal"])
print("Classification Report:\n",report)

precision=precision_score(y_true_all,y_pred_all,pos_label=1)
f1=f1_score(y_true_all, y_pred_all, pos_label=1)
recall=recall_score(y_true_all, y_pred_all, pos_label=1)
print(f"Precision (Abnormal): {precision:.3f}")
print(f"Recall (Abnormal): {recall:.3f}")
print(f"F1-score (Abnormal): {f1:.3f}")


anomaly_scores=np.array([0.4,0.88,0.123,0.9,0.22,0.7,0.1,0.56])
ground_truth=np.array([0,1,1,0,1,0,1,1])
plt.hist(anomaly_scores, bins=20, color="skyblue", edgecolor="black")
plt.xlabel("anomaly_scores")
plt.ylabel("clip count")
plt.title("Distribution of anomalies")
plt.show()



from sklearn.calibration import calibration_curve
pred_probs = model.predict(test_seq)
pred_probs = pred_probs.flatten()
prob_true, prob_pred = calibration_curve(y_true_all, pred_probs, n_bins=10)

plt.figure(figsize=(8, 5))
plt.plot(prob_pred, prob_true, marker='o', label='Calibration curve')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')
plt.xlabel('predition percentage')
plt.ylabel('true probability')
plt.title('Predictive Calibration Curve')
plt.legend()
plt.show()
from sklearn.metrics import brier_score_loss

brier = brier_score_loss(y_true_all, pred_probs)
print("Brier scores:", brier)




